{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminramezani345/Text-Information-Extraction/blob/main/scripts/xml2sentence_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftcArsPV9atT"
      },
      "source": [
        "# XML 2 Sentence Format Demo\n",
        "\n",
        "This notebook is for showing how to convert the XML format to sentences for training in other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mBbN6f439atW",
        "outputId": "f1455977-dbcc-4638-cdc7-8a55254af50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7199041615b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmedtator_kits\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_kits\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# force reload everything in mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'medtator_kits'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import medtator_kits as mtk\n",
        "import sentence_kits as stk\n",
        "\n",
        "# force reload everything in mtk\n",
        "import importlib\n",
        "importlib.reload(mtk)\n",
        "importlib.reload(stk)\n",
        "\n",
        "import copy\n",
        "import random\n",
        "\n",
        "# for data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for display nicer\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# load sentence detection\n",
        "import pysbd\n",
        "# load spacy and config the sentencizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.tokenizer\n",
        "\n",
        "# for hiding the warnings\n",
        "# please remove this for debugging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('* loaded all libraries')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQ-qRsu9atX"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tlJtZyvv9atY",
        "outputId": "d37b9625-5073-4547-c4bd-67557718be4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b4daa840f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_xmls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mtk' is not defined"
          ]
        }
      ],
      "source": [
        "path = '../sample/ENTITY_RELATION_TASK/ann_xml/Annotator_A/'\n",
        "rst = mtk.parse_xmls(path)\n",
        "print(rst['stat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBHoEEER9atY"
      },
      "source": [
        "## Parse and convert format\n",
        "\n",
        "We want to convert the text into a sentence-based format for downstream task (e.g., training relation extraction), so first of all, a sentencizer and a tokenizer are needed.\n",
        "You can use any libraries for this purpose.\n",
        "\n",
        "For here, we use `pySBD` for [sentence boundary detection](https://github.com/nipunsadvilkar/pySBD).\n",
        "\n",
        "```Python\n",
        "import pysbd\n",
        "text = \"My name is Jonas E. Smith. Please turn to p. 55.\"\n",
        "seg = pysbd.Segmenter(language=\"en\", clean=False, char_span=True)\n",
        "print(seg.segment(text))\n",
        "# [TextSpan(sent='My name is Jonas E. Smith. ', start=0, end=27), TextSpan(sent='Please turn to p. 55.', start=27, end=48)]\n",
        "```\n",
        "\n",
        "and we use `spaCy`'s [Tokenizer](https://spacy.io/api/tokenizer).\n",
        "\n",
        "```Python\n",
        "\n",
        "# Use Sentencizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "tokens = tokenizer(\"This is a sentence for tokens.\")\n",
        "sentence_tokens = list(map(lambda v: v.text, tokens))\n",
        "sentence_tokens\n",
        "# ['This', 'is', 'a', 'sentence', 'for', 'tokens', '.']\n",
        "```\n",
        "\n",
        "Second, we need a way to map the spans to token index.\n",
        "In the `sentence_kits.py`, we implemented a function `update_ents_token_index()` for this purpose. It uses the spans of a tag to check whether overlapped with any tokens of a sentence.\n",
        "The function `update_ents_token_index()` will update the entities by adding a new property `token_index` which is a list of token indexes.\n",
        "\n",
        "For more details, please check the follwoing section *Sentence-based format* and the source code in `sentence_kits.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErL90hWo9atZ"
      },
      "source": [
        "### Sentence-based format\n",
        "\n",
        "In the following demo, we will convert each XML file into a sentence-based format, which looks like the following:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"text\": \"The full text of the file\",\n",
        "    \"sentence_tags\": [{\n",
        "        \"sentence\": \"this is a sentence.\",\n",
        "        \"sentence_tokens\": [\"this\", \"is\", \"a\" \"sentence\", \".\"],\n",
        "        \"spans\": [start, end],\n",
        "        \"entities\": {\n",
        "            \"A1\": {\n",
        "                \"id\": \"A1\",\n",
        "                \"text\": \"this\",\n",
        "                \"token_index\": [0, 0]\n",
        "                // other properties\n",
        "            },\n",
        "            \"A2\": {\n",
        "                \"id\": \"A2\",\n",
        "                \"text\": \"a sentence\",\n",
        "                \"token_index\": [2, 3]\n",
        "                // other properties\n",
        "            }\n",
        "        },\n",
        "        \"relations\": {\n",
        "            \"R1\": {\n",
        "                \"id\": \"R1\",\n",
        "                \"link_EAID\": \"A1\", // this\n",
        "                \"link_EBID\": \"A2\", // a sentence\n",
        "            }\n",
        "        }\n",
        "    }]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-fR6pQy9atZ"
      },
      "outputs": [],
      "source": [
        "# let's check the sentence-based format for the given samples\n",
        "ann_sents = stk.convert_anns_to_sentags(rst['anns'])\n",
        "print(\"* got %s ann_sents\" % (len(ann_sents)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZTe3G-c9atZ"
      },
      "outputs": [],
      "source": [
        "# let's show how the results look like\n",
        "# the following code just for reference, you can use and modify for your purpose\n",
        "for ann_idx, (ann, ann_sent) in enumerate(zip(rst['anns'], ann_sents)):\n",
        "    print('*' * 30, ann['_filename'], len(ann_sent['sentence_tags']), 'sent(s)', '*' * 30)\n",
        "    for sentag in ann_sent['sentence_tags']:\n",
        "        if len(sentag['relations'])>0: sign = '<b style=\"color:red;\">HAS REL</b>:'\n",
        "        else: sign = 'ENT ONLY:'\n",
        "\n",
        "        # parse the tokens\n",
        "        tokens = copy.copy(sentag['sentence_tokens'])\n",
        "        # print(tokens)\n",
        "        for ent in sentag['entities'].values():\n",
        "            color = ''.join([random.choice('9ABCDEF') for j in range(6)])\n",
        "            for idx in range(ent['token_index'][0], ent['token_index'][1] + 1):\n",
        "                tokens[idx] = '<span style=\"color:#%s;background:black;\">%s</span>' % (\n",
        "                    color,\n",
        "                    tokens[idx]\n",
        "                )\n",
        "        display(HTML(sign + \"[ `\" + \"` | `\".join(tokens) + \"`]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV5wG2ZC9ata"
      },
      "source": [
        "As you can see, each token is shown in a pair of ``. \n",
        "The entities are located by the token index.\n",
        "\n",
        "Then, we can use this way to explore the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6zAZyo29atb"
      },
      "source": [
        "# Demo 1: Adverse event severity relation detection\n",
        "\n",
        "In this demo, we use the toolkits in this folder to make a tiny dataset for training a model for detection severity of adverse event.\n",
        "\n",
        "Please run the above cells to load the sample dataset.\n",
        "After loading, all documents are loaded into a variable `ann_sents`.\n",
        "Now, let's convert the data to a tiny training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1COM6Tpc9atb"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "First of all, we can create a tiny dataset from the annotated corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faOzeDFw9atc"
      },
      "outputs": [],
      "source": [
        "ds = []\n",
        "\n",
        "# the prop prefix for the Adverse Event and Severity\n",
        "prop_AE = 'link_AE'\n",
        "prop_SVRT = 'link_SVRT'\n",
        "\n",
        "for ann_idx, (ann_sent, ann) in enumerate(zip(ann_sents, rst['anns'])):\n",
        "    # ok, for each annotation file, \n",
        "    # we want to extract the annotated relations for positive training dataset\n",
        "    # there can be multiple sentence in a file, \n",
        "    # so we need to check each sentence\n",
        "    for sentag in ann_sent['sentence_tags']:\n",
        "        if len(sentag['relations'])==0:\n",
        "            # Oh, this sentence doesn't have a relation annotated\n",
        "            # but we can use this sentence for building negative samples\n",
        "            # as the SVRT tags and AE tags have no relations\n",
        "            # this is just a demo, please change it accordingly.\n",
        "            stags_ae = []\n",
        "            stags_svrt = []\n",
        "            for tag in sentag['entities'].values():\n",
        "                if tag['tag'] == 'AE': stags_ae.append(tag)\n",
        "                elif tag['tag'] == 'SVRT': stags_svrt.append(tag)\n",
        "            # let's check how many tags \n",
        "            # but if not AE or SVRT tags, we just skip\n",
        "            if len(stags_ae)==0 or len(stags_svrt)==0: continue\n",
        "\n",
        "            # OK, pair each ae and svrt as NEGATIVE sample\n",
        "            for _tag_a in stags_ae:\n",
        "                for _tag_s in stags_svrt:\n",
        "                    # create a data item\n",
        "                    d = {\n",
        "                        \"AE\": _tag_a,\n",
        "                        \"SVRT\": _tag_s,\n",
        "                        \"sentence\": sentag['sentence'],\n",
        "                        \"tokens\": sentag['sentence_tokens'],\n",
        "                        \"ann_idx\": ann_idx,\n",
        "                        \"y\": 0, # for those obtained by creating, define as 0\n",
        "                    }\n",
        "\n",
        "                    ds.append(d)\n",
        "\n",
        "        else:\n",
        "            # Ok, this sentence may have several relations\n",
        "            # let's check one by one\n",
        "            for rel in sentag['relations'].values():\n",
        "                # get the AE\n",
        "                ent_ae = sentag['entities'][rel['%sID' % prop_AE]]\n",
        "                ent_svrt = sentag['entities'][rel['%sID' % prop_SVRT]]\n",
        "\n",
        "                # ok, we can save this relation now\n",
        "                d = {\n",
        "                    \"AE\": ent_ae,\n",
        "                    \"SVRT\": ent_svrt,\n",
        "                    \"sentence\": sentag['sentence'],\n",
        "                    \"tokens\": sentag['sentence_tokens'],\n",
        "                    \"ann_idx\": ann_idx,\n",
        "                    \"y\": 1, # for those obtained from relation, define as 1(POSITIVE)\n",
        "                }\n",
        "\n",
        "                ds.append(d)\n",
        "\n",
        "df = pd.DataFrame(ds)\n",
        "print('* got the dataset %d records!' % (len(df)))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwjextQr9atc"
      },
      "source": [
        "## Get features\n",
        "\n",
        "Although we have got all the information needed, we still need to convert the information to features which can be read by machine.\n",
        "\n",
        "There are so many methods to convert, simple and complex. You will never know which one is the best if you don't try and understand the pros and cons of each method. For this demo purpose, we just use a very simple way to get two types of features:\n",
        "\n",
        "1. the number of tokens between AE and severity\n",
        "2. TF-IDF feature of the whole sentence\n",
        "\n",
        "In practice settings, the above features should be optimized to reduce noises, such as stopwords and special symbols. Moreover, you may choose BERT-based models to get text embeddings of the tokens, POS features, and other knowledge graph embeddings to fully capture the information. We plan to include more practical demos to show how to do that in future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TE6MLX_9atd"
      },
      "source": [
        "### Get between tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXXzfKkQ9atd"
      },
      "outputs": [],
      "source": [
        "def get_between_tokens(x):\n",
        "    '''\n",
        "    Get the tokens between AE and SVRT\n",
        "    '''\n",
        "    left_idx = x['AE']['token_index']\n",
        "    right_idx = x['SVRT']['token_index']\n",
        "\n",
        "    if left_idx[0] > right_idx[1]:\n",
        "        # which means AE is after SVRT\n",
        "        left_idx = x.SVRT['token_index']\n",
        "        right_idx = x.AE['token_index']\n",
        "\n",
        "    # get the tokens between left and right\n",
        "    between_tokens = x.tokens[\n",
        "        left_idx[1] + 1 : right_idx[0]\n",
        "    ]\n",
        "\n",
        "    return between_tokens\n",
        "\n",
        "\n",
        "def get_HTML(x):\n",
        "    tokens = x.tokens\n",
        "    for idx in range(x.AE['token_index'][0], x.AE['token_index'][1] + 1):\n",
        "        tokens[idx] = \"<span style='color:blue'>%s</span>\" % tokens[idx]\n",
        "    for idx in range(x.SVRT['token_index'][0], x.SVRT['token_index'][1] + 1):\n",
        "        tokens[idx] = \"<span style='color:red'>%s</span>\" % tokens[idx]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['between_tokens'] = df.apply(lambda r: get_between_tokens(r), axis=1)\n",
        "    \n",
        "# normalize as 10 tokens\n",
        "df['n_bt'] = df['between_tokens'].apply(lambda ts: len(ts)/10 if len(ts)/10 < 1 else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7GQge_l9ate"
      },
      "source": [
        "### Get TF-IDF\n",
        "\n",
        "The `tfvecter` is needed for future use, you can also save it by pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCaFw-n9ate"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfvecter = TfidfVectorizer()\n",
        "tfidf = tfvecter.fit_transform(df['sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ktqkxlc9ate"
      },
      "source": [
        "### Convert to feature vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hV63SefJ9ate",
        "outputId": "bbc822ca-47e8-477e-fcd2-237cf83467bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-78daa34db537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_bt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# flatten the tf-idf column as a df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# concat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "df_dataset = df[['y', 'n_bt']]\n",
        "\n",
        "# flatten the tf-idf column as a df\n",
        "df_tfidf = pd.DataFrame(tfidf.toarray(), index=df.index, columns=['t%d' % i for i in range(tfidf.shape[1])])\n",
        "# concat\n",
        "df_dataset = pd.concat([\n",
        "    df_dataset,\n",
        "    df_tfidf\n",
        "], axis=1)\n",
        "\n",
        "df_dataset.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cUMHZtW9atf"
      },
      "source": [
        "### Split train/test\n",
        "As you can see, a very tiny dataset is created with both positive and negative samples. And we preserved as much information as possible in the dataframe to generate features. \n",
        "\n",
        "In practice settings, you don't need to save those raw data into a dataframe. Instead, you can just save major information, such as token, index, sentences, which can reduce the space needed for large corpus.\n",
        "\n",
        "You can save this dataset into `pkl` format for futuer usage as follows or any other formats.\n",
        "\n",
        "```Python\n",
        "import pickle\n",
        "\n",
        "with open('dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)\n",
        "```\n",
        "\n",
        "In future, you can load the dataset as follows:\n",
        "\n",
        "```Python\n",
        "with open('dataset.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "```\n",
        "\n",
        "Now, let's get the training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8q49PDf9atf"
      },
      "outputs": [],
      "source": [
        "# random select 80% for training\n",
        "df_train = df_dataset.sample(frac=0.8)\n",
        "# the rest 20% for test\n",
        "df_test = df_dataset.iloc[~df_dataset.index.isin(df_train.index)]\n",
        "\n",
        "# get the value s \n",
        "X_train = df_train.loc[:, df_train.columns != 'y'].to_numpy()\n",
        "y_train = df_train.loc[:, 'y'].to_numpy()\n",
        "\n",
        "X_test = df_test.loc[:, df_test.columns != 'y'].to_numpy()\n",
        "y_test = df_test.loc[:, 'y'].to_numpy()\n",
        "\n",
        "print('* got df_train', len(df_train))\n",
        "print('* got df_test', len(df_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkSVUEqK9atf"
      },
      "source": [
        "## Build model\n",
        "\n",
        "In fact, there are so many model available in the machine learning world.\n",
        "You can choose whatever you want for a project.\n",
        "For here, we just use a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) which is easy to get and good for high-dimensional datasets.\n",
        "\n",
        "In practice settings, building model has many options. Try to enjoy it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azkucTfQ9atg"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(max_depth=10, random_state=0)\n",
        "print('* created a RFC model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNXmO2v9atg"
      },
      "source": [
        "## Train model\n",
        "\n",
        "We can train the model on the given training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLx2G3cf9atg"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train)\n",
        "print('* trained model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7rs8U39atg"
      },
      "source": [
        "## Evaluate model\n",
        "\n",
        "Then we can get the reports on the model based on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t1CtMWa9ath"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# get the prediction value\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Accuracy     : %.3f'%accuracy_score(y_test, y_pred))\n",
        "print('Precision    : %.3f'%precision_score(y_test, y_pred))\n",
        "print('Recall       : %.3f'%recall_score(y_test, y_pred))\n",
        "print('F1-Score     : %.3f'%f1_score(y_test, y_pred))\n",
        "print('\\nClassification Report : ')\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euxOP39N9ath"
      },
      "source": [
        "## Apply model\n",
        "\n",
        "Now, once you got this model, you can put it into practice! (yes, this is a toy for demo. You need to put a *REAL* model into practice for sure).\n",
        "\n",
        "Usually, you can save the model as a pickle file and use it in future:\n",
        "\n",
        "```Python\n",
        "# save model\n",
        "filename = 'final_model.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# load model\n",
        "model = pickle.load(open(filename, 'rb'))\n",
        "```\n",
        "\n",
        "In addition, as this tiny uses TF-IDF for features, the `tfvecter` is needed to get the same dimensional features. \n",
        "You can also load the `tfvecter` before application.\n",
        "\n",
        "### Preprocessing\n",
        "\n",
        "Before start using the trained model into practice, please keep in mind that the input must be the **same format** of training phase, which means it needs to preprocess the data to get the AE and SVRT from the given text first.\n",
        "\n",
        "This preprocess can be done by a Named Entity Recognition (NER) model automatically.\n",
        "But you need to pair the AE and SVRT found by the NER model by another algorithm.\n",
        "For example, if the NER model finds 3 AEs and 2 SVRTs, you can just pair all 6 possible combinations as the candidates:\n",
        "\n",
        "$$\n",
        "AE_1 + SVRT_1 |\n",
        "AE_1 + SVRT_2 |\n",
        "AE_2 + SVRT_1 |\n",
        "AE_2 + SVRT_2 |\n",
        "AE_3 + SVRT_1 |\n",
        "AE_3 + SVRT_2\n",
        "$$\n",
        "\n",
        "Then, let the model to decide which one is correct. \n",
        "In fact, there are also some strategies to reduce the number of the candidates.\n",
        "\n",
        "For here the demo, we will skip the NER process and just show the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwsbAacr9ath"
      },
      "outputs": [],
      "source": [
        "# the AE and SVRT\n",
        "df_sample = pd.DataFrame([[\n",
        "    'headache', 'mild', [',', 'but', 'still'], \n",
        "    '2/20/21:  2AM advil helped the headache, but still mild diarrhea.'\n",
        "], [\n",
        "    'diarrhea', 'mild', [], \n",
        "    '2/20/21:  2AM advil helped the headache, but still mild diarrhea.'\n",
        "], [\n",
        "    'nause', 'Mild', [],\n",
        "    'Mild nausea beginning the next day (3/7) and lasting until (3/8).'\n",
        "], [\n",
        "    'dizziness', 'intense', ['headache','yesterday',',','but','after','a','few','days',',','I','only','felt','some'],\n",
        "    'I got an intense headache yesterday, but after a few days, I only felt some dizziness.'\n",
        "]], columns=['AE', 'SVRT', 'between_tokens', 'sentence'])\n",
        "\n",
        "# First, convert to features\n",
        "# 1. n_bt\n",
        "f_n_bt = df_sample['between_tokens'].apply(lambda ts: len(ts)/10 if len(ts)/10 < 1 else 1)\n",
        "f_n_bt = f_n_bt.to_numpy().reshape([len(f_n_bt), 1])\n",
        "print('* f_n_bt shape', f_n_bt.shape)\n",
        "\n",
        "# 2. tf-idf\n",
        "f_tfidf = tfvecter.transform(df_sample.sentence).toarray()\n",
        "print('* f_tfidf shape', f_tfidf.shape)\n",
        "\n",
        "# concat n_bt and tf-idf\n",
        "X_sample = np.concatenate([f_n_bt, f_tfidf], axis=1)\n",
        "print('* X_sample shape', X_sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGdvpxLP9ati"
      },
      "source": [
        "### Run model\n",
        "\n",
        "Instead of predicting the class/label directly with `predict()`, we use `predict_proba()` to show the class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UknN-buS9ati"
      },
      "outputs": [],
      "source": [
        "y_sample = model.predict_proba(X_sample)\n",
        "\n",
        "# let's see what's the result:\n",
        "for i, r in df_sample.iterrows():\n",
        "    print(\"RESULT: %s | [%s] - [%s] | %s\" % (\n",
        "        y_sample[i], r['AE'], r['SVRT'], r['sentence']\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQaa5Iw_9ati"
      },
      "source": [
        "As you can see above, the second and third sample show high probability, which indicates the relation is likely to be positive.\n",
        "The first and the last one show lower probability, which indicates the relation may be negative.\n",
        "Well, as this is just a simple demo, a rough threshold can be set as 0.8 or higher for the relation decision.\n",
        "\n",
        "You can improve the performance for real production in many aspect, such as:\n",
        "\n",
        "1. **A large annotated dataset**. A well-annotated dataset is always helpful for improving performance.\n",
        "2. **Feature engineering**. Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in machine learning. Good features can not only improve the overall performance, but also make the follow-up steps easier.\n",
        "3. **Nice models**. Contrary to what many believe, the machine learning model with the best performance is not necessarily the best solution. Choosing a nice model is not easy for practice, you need to balance the cost and performance. Some SOTA models have very good performance but need more computation power, while some may have acceptable performance with less cost (e.g., Random Forest is widely used in industries)\n",
        "4. **Hyperparameter tuning**. How you train the model is also important. There are some methods to optimize the hyperparameters in both open-source solutions and commercial solutions.\n",
        "5. **Problem definition**. This should be first considered for sure. It's out of the scope of this notebook.\n",
        "\n",
        "Well, to the best of my knowledge, it's the basic workflow "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLPy37",
      "language": "python",
      "name": "nlpy37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}